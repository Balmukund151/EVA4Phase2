{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnuRwqnacxcm",
        "outputId": "580ba2ab-8a5a-4d43-ae63-b9032d36779d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrltsT6Ic4-L",
        "outputId": "ad8f3b17-5e06-46e0-9da5-14e911298f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!git clone https://github.com/leftthomas/SRGAN.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'SRGAN' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF2nmOtwf9Kn",
        "outputId": "a38a0ac8-9496-4179-907e-a6b4fe99b146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%cd \"/content/SRGAN\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SRGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTA-c9SrFNhh",
        "outputId": "bed9c8d6-ea69-4bb6-e29e-f1b63b207b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "!python train.py --crop_size 32 --num_epochs 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# generator parameters: 734219\n",
            "# discriminator parameters: 5215425\n",
            "[1/20] Loss_D: 0.8138 Loss_G: 0.0260 D(x): 0.8424 D(G(z)): 0.6579: 100% 64/64 [00:54<00:00,  1.18it/s]\n",
            "[converting LR images to SR images] PSNR: 17.9301 dB SSIM: 0.5507: 100% 1026/1026 [02:46<00:00,  6.17it/s]\n",
            "tcmalloc: large alloc 5909766144 bytes == 0x7fad5ce14000 @  0x7fb14a2a0b6b 0x7fb14a2c0379 0x7fb0eed5192e 0x7fb0eed53946 0x7fb12712fbb3 0x7fb126be417a 0x7fb126be5293 0x7fb1270e5edf 0x7fb126c736a6 0x7fb126c747b0 0x7fb126ecb1c9 0x7fb12686c689 0x7fb126faa919 0x7fb126c7609b 0x7fb127060a08 0x7fb12686c689 0x7fb126faa709 0x7fb126c6bd1d 0x7fb12706adc8 0x7fb12686c689 0x7fb126faab29 0x7fb128ce4347 0x7fb12686c689 0x7fb126faab29 0x7fb1366445b6 0x50a7f5 0x50c1f4 0x507f24 0x50b053 0x634dd2 0x634e87\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vFbF6YY0ydV"
      },
      "source": [
        "import PIL\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6v0QAlIFNlB"
      },
      "source": [
        "import cv2,os\n",
        "path=\"/content/gdrive/My Drive/SR/finished/valid/dataraw/hires\"\n",
        "for file in os.listdir(path):\n",
        "    img=PIL.Image.open(os.path.join(path,file))\n",
        "    shp=np.array(img).shape\n",
        "    if len(shp) is not 3 or shp[2]>3 or shp[2]==1:\n",
        "        print(\"deleting \", file)\n",
        "        os.remove(os.path.join(path,file))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36WrqEMIu2AQ",
        "outputId": "40d89339-89a2-41ef-c096-cb70ac1a273a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "from math import log10\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as utils\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pytorch_ssim\n",
        "from data_utils import TrainDatasetFromFolder, ValDatasetFromFolder, display_transform\n",
        "from loss import GeneratorLoss\n",
        "from model import Generator, Discriminator\n",
        "\n",
        "if 1 == 1:\n",
        "    #opt = parser.parse_args()\n",
        "    \n",
        "    CROP_SIZE = 32\n",
        "    UPSCALE_FACTOR = 4\n",
        "    NUM_EPOCHS = 5\n",
        "    \n",
        "    train_set = TrainDatasetFromFolder('/content/gdrive/My Drive/SR/finished/train/dataraw/hires', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "    val_set = ValDatasetFromFolder('/content/gdrive/My Drive/SR/finished/test', upscale_factor=UPSCALE_FACTOR)\n",
        "    train_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(dataset=val_set, num_workers=0, batch_size=1, shuffle=False)\n",
        "    \n",
        "    netG = Generator(UPSCALE_FACTOR)\n",
        "    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
        "    netD = Discriminator()\n",
        "    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
        "    \n",
        "    generator_criterion = GeneratorLoss()\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        netG.cuda()\n",
        "        netD.cuda()\n",
        "        generator_criterion.cuda()\n",
        "    \n",
        "    optimizerG = optim.Adam(netG.parameters())\n",
        "    optimizerD = optim.Adam(netD.parameters())\n",
        "    \n",
        "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
        "    \n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        train_bar = tqdm(train_loader)\n",
        "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
        "        torch.autograd.set_detect_anomaly(True)\n",
        "        netG.train()\n",
        "        netD.train()\n",
        "        for data, target in train_bar:\n",
        "            g_update_first = True\n",
        "            batch_size = data.size(0)\n",
        "            running_results['batch_sizes'] += batch_size\n",
        "    \n",
        "            ############################\n",
        "            # (1) Update D network: maximize D(x)-1-D(G(z))\n",
        "            ###########################\n",
        "            real_img = Variable(target)\n",
        "            if torch.cuda.is_available():\n",
        "                real_img = real_img.cuda()\n",
        "            z = Variable(data)\n",
        "            if torch.cuda.is_available():\n",
        "                z = z.cuda()\n",
        "            fake_img = netG(z)\n",
        "    \n",
        "            netD.zero_grad()\n",
        "            real_out = netD(real_img).mean()\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            d_loss = 1 - real_out + fake_out\n",
        "            d_loss.backward(retain_graph=True)\n",
        "            optimizerD.step()\n",
        "    \n",
        "            ############################\n",
        "            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
        "            ###########################\n",
        "            netG.zero_grad()\n",
        "            fake_img = netG(z)\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "            g_loss.backward()\n",
        "            \n",
        "            fake_img = netG(z)\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            \n",
        "            \n",
        "            optimizerG.step()\n",
        "\n",
        "            # loss for current batch before optimization \n",
        "            running_results['g_loss'] += g_loss.item() * batch_size\n",
        "            running_results['d_loss'] += d_loss.item() * batch_size\n",
        "            running_results['d_score'] += real_out.item() * batch_size\n",
        "            running_results['g_score'] += fake_out.item() * batch_size\n",
        "    \n",
        "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "                running_results['g_loss'] / running_results['batch_sizes'],\n",
        "                running_results['d_score'] / running_results['batch_sizes'],\n",
        "                running_results['g_score'] / running_results['batch_sizes']))\n",
        "    \n",
        "        netG.eval()\n",
        "        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
        "        if not os.path.exists(out_path):\n",
        "            os.makedirs(out_path)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader)\n",
        "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
        "            val_images = []\n",
        "            count=0\n",
        "            for val_lr, val_hr_restore, val_hr in val_bar:\n",
        "                #if count>400:\n",
        "                  #print(\"breaking out as counter reached \",count)\n",
        "                  #break\n",
        "                batch_size = val_lr.size(0)\n",
        "                #count=count+1\n",
        "                valing_results['batch_sizes'] += batch_size\n",
        "                lr = val_lr\n",
        "                hr = val_hr\n",
        "                if torch.cuda.is_available():\n",
        "                    lr = lr.cuda()\n",
        "                    hr = hr.cuda()\n",
        "                sr = netG(lr)\n",
        "        \n",
        "                batch_mse = ((sr - hr) ** 2).data.mean()\n",
        "                valing_results['mse'] += batch_mse * batch_size\n",
        "                batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
        "                valing_results['ssims'] += batch_ssim * batch_size\n",
        "                valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
        "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
        "                val_bar.set_description(\n",
        "                    desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
        "                        valing_results['psnr'], valing_results['ssim']))\n",
        "        \n",
        "                val_images.extend(\n",
        "                    [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
        "                     display_transform()(sr.data.cpu().squeeze(0))])\n",
        "            val_images = torch.stack(val_images)\n",
        "            val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
        "            val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
        "            index = 1\n",
        "            for image in val_save_bar:\n",
        "                image = utils.make_grid(image, nrow=3, padding=5)\n",
        "                utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
        "                index += 1\n",
        "    \n",
        "        # save model parameters\n",
        "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "        # save loss\\scores\\psnr\\ssim\n",
        "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
        "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
        "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
        "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
        "        results['psnr'].append(valing_results['psnr'])\n",
        "        results['ssim'].append(valing_results['ssim'])\n",
        "    \n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            out_path = 'statistics/'\n",
        "            data_frame = pd.DataFrame(\n",
        "                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
        "                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
        "                index=range(1, epoch + 1))\n",
        "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# generator parameters: 734219\n",
            "# discriminator parameters: 5215425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[1/5] Loss_D: 0.9832 Loss_G: 0.0217 D(x): 0.6136 D(G(z)): 0.5883: 100%|██████████| 128/128 [02:17<00:00,  1.08s/it]\n",
            "[converting LR images to SR images] PSNR: 17.6274 dB SSIM: 0.5802: 100%|██████████| 14/14 [00:11<00:00,  1.20it/s]\n",
            "[saving training results]: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n",
            "[2/5] Loss_D: 1.0019 Loss_G: 0.0125 D(x): 0.7246 D(G(z)): 0.7251: 100%|██████████| 128/128 [02:14<00:00,  1.05s/it]\n",
            "[converting LR images to SR images] PSNR: 19.6375 dB SSIM: 0.6234: 100%|██████████| 14/14 [00:04<00:00,  2.93it/s]\n",
            "[saving training results]: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]\n",
            "[3/5] Loss_D: 1.0001 Loss_G: 0.0093 D(x): 0.9736 D(G(z)): 0.9751: 100%|██████████| 128/128 [02:15<00:00,  1.06s/it]\n",
            "[converting LR images to SR images] PSNR: 17.9070 dB SSIM: 0.6625: 100%|██████████| 14/14 [00:04<00:00,  2.95it/s]\n",
            "[saving training results]: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]\n",
            "[4/5] Loss_D: 1.0000 Loss_G: 0.0080 D(x): 0.9999 D(G(z)): 0.9999: 100%|██████████| 128/128 [02:15<00:00,  1.06s/it]\n",
            "[converting LR images to SR images] PSNR: 19.0084 dB SSIM: 0.6447: 100%|██████████| 14/14 [00:04<00:00,  2.96it/s]\n",
            "[saving training results]: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]\n",
            "[5/5] Loss_D: 1.0000 Loss_G: 0.0072 D(x): 0.9999 D(G(z)): 0.9999: 100%|██████████| 128/128 [02:15<00:00,  1.06s/it]\n",
            "[converting LR images to SR images] PSNR: 20.3432 dB SSIM: 0.6895: 100%|██████████| 14/14 [00:04<00:00,  2.93it/s]\n",
            "[saving training results]: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}